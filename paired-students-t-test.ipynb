{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.axes._axes import Axes  # For typing purposes\n",
    "import numpy as np\n",
    "import pyperf\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(result: Dict[str, Any]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Parse data\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for benchmark in result[\"benchmarks\"]:\n",
    "        if \"metadata\" in benchmark:\n",
    "            name = benchmark[\"metadata\"][\"name\"]\n",
    "        else:\n",
    "            name = result[\"metadata\"][\"name\"]\n",
    "        data = []\n",
    "        for run in benchmark[\"runs\"]:\n",
    "            data.extend(run.get(\"values\", []))\n",
    "        results[name] = np.array(data, dtype=np.float64)\n",
    "        results[name].sort()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "run = \"run6\"\n",
    "\n",
    "file1 = \"python3-11-3-low-01\"\n",
    "file2 = \"python3-11-3-low-02\"\n",
    "\n",
    "file1_path = f\"results/{run}/bm-{file1}.json\"\n",
    "file2_path = f\"results/{run}/bm-{file2}.json\"\n",
    "\n",
    "with open(file1_path) as fb, open(file2_path) as fh:\n",
    "    run1 = get_data(json.load(fb))\n",
    "    run2 = get_data(json.load(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that benchmarks match\n",
    "if run1.keys() != run2.keys():\n",
    "    raise Exception(\"The benchmarking suites are not the same size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix data\n",
    "for name in run1.keys():\n",
    "    temp1 = np.concatenate([run1[name][0::2], run2[name][1::2]])\n",
    "    temp1.sort()\n",
    "    temp2 = np.concatenate([run2[name][0::2], run1[name][1::2]])\n",
    "    temp2.sort()\n",
    "\n",
    "    run1[name] = temp1\n",
    "    run2[name] = temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive outlier removal\n",
    "for name in run1.keys():\n",
    "    run1[name] = run1[name][:45]\n",
    "    run2[name] = run2[name][:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate diffs\n",
    "diffs = {}\n",
    "\n",
    "for name in run1.keys():\n",
    "    diffs[name] = run1[name] - run2[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dist_plot(ax: Axes, data: np.ndarray) -> None:\n",
    "    # The density set to `True` makes the integral of the histogram 1.\n",
    "    ax.hist(data, alpha=0.5, density=True, bins=20)\n",
    "    ax.xaxis.set_major_formatter(lambda val, _: f\"{val:4.1g}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(ax: Axes, data: np.ndarray, t_score: float) -> None:\n",
    "    r_labels = [\n",
    "        \"Minimum\",\n",
    "        \"Maximum\",\n",
    "        \"Mean\",\n",
    "        \"Variance\",\n",
    "        \"T-Score\",\n",
    "        \"Significant\",\n",
    "    ]\n",
    "\n",
    "    data = [\n",
    "        [data.min()],\n",
    "        [data.max()],\n",
    "        [data.mean()],\n",
    "        [data.var()],\n",
    "        [t_score],\n",
    "        [True if abs(t_score) > 2 else False],\n",
    "    ]\n",
    "    data = [[f\"{b[0]:4.2g}\"] for b in data]  # Round to 4 decimal points\n",
    "\n",
    "    table = ax.table(cellText=data, loc='center', cellLoc='center', rowLabels=r_labels, colWidths=[.4])\n",
    "    table.set_fontsize(12)\n",
    "\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    fig, axs = pyplot.subplots(3, 2, figsize=(12, 3 * 3), layout=\"constrained\", width_ratios=[.6, .3])\n",
    "else:\n",
    "    fig, axs = pyplot.subplots(len(run1), 2, figsize=(12, 3 * len(run1)), layout=\"constrained\", width_ratios=[.6, .3])\n",
    "\n",
    "# Sort by t-score\n",
    "pairs = []\n",
    "significant = 0\n",
    "for name in run1.keys():\n",
    "    t_score, p_val = stats.ttest_rel(run1[name], run2[name])\n",
    "    pairs.append((name, t_score))\n",
    "    if abs(t_score) >= 2:\n",
    "        significant += 1\n",
    "pairs = sorted(pairs, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "ax_counter = 0\n",
    "\n",
    "for name, t_score in pairs:\n",
    "    axs[ax_counter][0].set_title(name)\n",
    "    generate_dist_plot(axs[ax_counter][0], diffs[name])\n",
    "    generate_table(axs[ax_counter][1], diffs[name], t_score)\n",
    "\n",
    "    ax_counter += 1\n",
    "\n",
    "    if debug and ax_counter == 3:\n",
    "        break\n",
    "\n",
    "title = f\"\"\"Comparision of: {file1} and {file2}\n",
    "    Significant: {significant}, out of: {len(run1)}\n",
    "\"\"\"\n",
    "fig.suptitle(title, fontsize=16)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
